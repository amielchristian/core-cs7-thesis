{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 35126 files belonging to 5 classes.\n",
      "Found 53576 files belonging to 5 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amiel/.local/lib/python3.10/site-packages/vit_keras/utils.py:81: UserWarning: Resizing position embeddings from 7, 7 to 8, 8\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vit-b32\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 256, 256, 3)]     0         \n",
      "                                                                 \n",
      " embedding (Conv2D)          (None, 8, 8, 768)         2360064   \n",
      "                                                                 \n",
      " reshape_3 (Reshape)         (None, 64, 768)           0         \n",
      "                                                                 \n",
      " class_token (ClassToken)    (None, 65, 768)           768       \n",
      "                                                                 \n",
      " Transformer/posembed_input  (None, 65, 768)           49920     \n",
      "  (AddPositionEmbs)                                              \n",
      "                                                                 \n",
      " Transformer/encoderblock_0  ((None, 65, 768),         7087872   \n",
      "  (TransformerBlock)          (None, 12, None, None)             \n",
      "                             )                                   \n",
      "                                                                 \n",
      " Transformer/encoderblock_1  ((None, 65, 768),         7087872   \n",
      "  (TransformerBlock)          (None, 12, None, None)             \n",
      "                             )                                   \n",
      "                                                                 \n",
      " Transformer/encoderblock_2  ((None, 65, 768),         7087872   \n",
      "  (TransformerBlock)          (None, 12, None, None)             \n",
      "                             )                                   \n",
      "                                                                 \n",
      " Transformer/encoderblock_3  ((None, 65, 768),         7087872   \n",
      "  (TransformerBlock)          (None, 12, None, None)             \n",
      "                             )                                   \n",
      "                                                                 \n",
      " Transformer/encoderblock_4  ((None, 65, 768),         7087872   \n",
      "  (TransformerBlock)          (None, 12, None, None)             \n",
      "                             )                                   \n",
      "                                                                 \n",
      " Transformer/encoderblock_5  ((None, 65, 768),         7087872   \n",
      "  (TransformerBlock)          (None, 12, None, None)             \n",
      "                             )                                   \n",
      "                                                                 \n",
      " Transformer/encoderblock_6  ((None, 65, 768),         7087872   \n",
      "  (TransformerBlock)          (None, 12, None, None)             \n",
      "                             )                                   \n",
      "                                                                 \n",
      " Transformer/encoderblock_7  ((None, 65, 768),         7087872   \n",
      "  (TransformerBlock)          (None, 12, None, None)             \n",
      "                             )                                   \n",
      "                                                                 \n",
      " Transformer/encoderblock_8  ((None, 65, 768),         7087872   \n",
      "  (TransformerBlock)          (None, 12, None, None)             \n",
      "                             )                                   \n",
      "                                                                 \n",
      " Transformer/encoderblock_9  ((None, 65, 768),         7087872   \n",
      "  (TransformerBlock)          (None, 12, None, None)             \n",
      "                             )                                   \n",
      "                                                                 \n",
      " Transformer/encoderblock_1  ((None, 65, 768),         7087872   \n",
      " 0 (TransformerBlock)         (None, 12, None, None)             \n",
      "                             )                                   \n",
      "                                                                 \n",
      " Transformer/encoderblock_1  ((None, 65, 768),         7087872   \n",
      " 1 (TransformerBlock)         (None, 12, None, None)             \n",
      "                             )                                   \n",
      "                                                                 \n",
      " Transformer/encoder_norm (  (None, 65, 768)           1536      \n",
      " LayerNormalization)                                             \n",
      "                                                                 \n",
      " ExtractToken (Lambda)       (None, 768)               0         \n",
      "                                                                 \n",
      " pre_logits (Dense)          (None, 768)               590592    \n",
      "                                                                 \n",
      " head (Dense)                (None, 5)                 3845      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 88061189 (335.93 MB)\n",
      "Trainable params: 88061189 (335.93 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-06 21:28:35.900540: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8907\n",
      "2024-07-06 21:28:36.023783: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-07-06 21:28:39.699446: I external/local_xla/xla/service/service.cc:168] XLA service 0x74ca19141c40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-07-06 21:28:39.699493: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1050 Ti with Max-Q Design, Compute Capability 6.1\n",
      "2024-07-06 21:28:39.708569: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1720272519.791548   35046 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1098/1098 [==============================] - ETA: 0s - loss: 3.8796 - accuracy: 0.1407"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from load_data import load_data\n",
    "from vit_keras import vit\n",
    "\n",
    "train_ds, val_ds = load_data()\n",
    "model = vit.vit_b32(\n",
    "    weights='imagenet21k',\n",
    "    image_size = 256,\n",
    "    pretrained=True,\n",
    "    pretrained_top=False,\n",
    "    classes=5\n",
    "    )\n",
    "model.summary()\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=2e-4),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    "    )\n",
    "model.fit(train_ds, validation_data=val_ds, epochs=10)\n",
    "model.save('baseline.keras')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
